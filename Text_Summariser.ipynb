{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Text Summariser.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMk6+NDzUPzVEBWMpJZHsl5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JoshitaReddy/Text-Summariser/blob/main/Text_Summariser.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Doewi1rNfPGD"
      },
      "source": [
        "Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9oH48hWfLoB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad354b4-8382-427c-a3b3-70f2fcc7f272"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import nltk\n",
        "nltk.download('punkt') \n",
        "nltk.download('stopwords')\n",
        "import re"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsUpOPX-m9Kv",
        "outputId": "4ff52c67-4132-4de2-84b7-8501794f641b"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove*.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-08 15:48:42--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-11-08 15:48:42--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-11-08 15:48:43--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 40s  \n",
            "\n",
            "2021-11-08 15:51:23 (5.14 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGokLqh7mRoC"
      },
      "source": [
        "Reading Text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqXbTzGsOSEb",
        "outputId": "fd8faa4c-7e62-4e97-ae5c-c50bb0886864"
      },
      "source": [
        "f = open(\"text.txt\", \"r\")\n",
        "text=f.read()\n",
        "print(text)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "We have already done great damage to Mother Earth. Now nations across the world have understood the urgency of the global warming issue, and are taking action to prevent further rising of global temperatures.\n",
            "\n",
            "There have been many agreements on ways to reduce the emission of greenhouse gases in the past.\n",
            "The latest one is the Paris agreement by the United Nations Framework Convention on Climate Change (UNFCCC) members.\n",
            "The main goal of this agreement is to keep the increase in global average temperature to well below 2 °C above pre-industrial levels and to limit the increase to 1.5 °C.\n",
            "To offset carbon emissions, the Indian government is taking various initiatives. These include the solar mission, electrification of transport, clean development mechanisms, effective utilization of natural resources such as wind energy, hydel power, etc.\n",
            "\n",
            "Our government has introduced schemes to reduce the usage of unclean fuels for cooking in rural households.\n",
            "As individuals, we too have the responsibility in making our planet a safe abode for our future generations.\n",
            "We should go for energy-efficient lights such as LED bulbs that don’t generate much heat. We can drive less and use public transport instead.\n",
            "\n",
            "Recycling has to become a habit in our society as the production of new components takes lots of energy and leads to pollution too.\n",
            "\n",
            "Make judicious use of natural resources such as water.\n",
            "Plant trees as a single tree can absorb one ton of carbon dioxide over its lifetime.\n",
            "We should also use fewer electronic devices for recreational purposes.\n",
            "Instead, it is better to go to beaches, parks, and zoos, so that we realize the importance of preserving nature’s gift to mankind.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mw5C8OTlmcmz"
      },
      "source": [
        "Text To Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPkIboKqmbwh"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "sentences = sent_tokenize(text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Apum-f18nMYM"
      },
      "source": [
        "Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gld2FX0fndhT"
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-PtWGgHinHjA"
      },
      "source": [
        "sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
        "stop_words = stopwords.words('english')\n",
        "sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVyyWikpn4wo"
      },
      "source": [
        "Vector Representation of Sentences"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AMWGsok8n7iJ"
      },
      "source": [
        "# Extract word vectors\n",
        "word_embeddings = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    coefs = np.asarray(values[1:], dtype='float32')\n",
        "    word_embeddings[word] = coefs\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUrZL9i2oCzt"
      },
      "source": [
        "sentence_vectors = []\n",
        "for i in sentences_clean:\n",
        "  if len(i) != 0:\n",
        "    v = sum([word_embeddings.get(w, np.zeros((100,))) for w in i.split()])/(len(i.split())+0.001)\n",
        "  else:\n",
        "    v = np.zeros((100,))\n",
        "  sentence_vectors.append(v)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXbDYKEmoGYv"
      },
      "source": [
        "Similarity Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_lZl3UGoFmN"
      },
      "source": [
        "# similarity matrix\n",
        "sim_mat = np.zeros([len(sentences), len(sentences)])\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ABk_6CAopHP_"
      },
      "source": [
        "for i in range(len(sentences)):\n",
        "  for j in range(len(sentences)):\n",
        "    if i != j:\n",
        "      sim_mat[i][j] = cosine_similarity(sentence_vectors[i].reshape(1,100), sentence_vectors[j].reshape(1,100))[0,0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXbkwsZApIF3"
      },
      "source": [
        "PageRank Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jYKHc8DKpMiI"
      },
      "source": [
        "import networkx as nx\n",
        "\n",
        "nx_graph = nx.from_numpy_array(sim_mat)\n",
        "scores = nx.pagerank(nx_graph)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skDM3T31pXk2"
      },
      "source": [
        "Extract Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WvuSYWKfpPNk"
      },
      "source": [
        "ranked_sentences = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oawrAzoNpS12",
        "outputId": "ae2e32bf-9369-432f-baa7-dd79ee981268"
      },
      "source": [
        "for i in range(5):\n",
        "  print(ranked_sentences[i][1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Recycling has to become a habit in our society as the production of new components takes lots of energy and leads to pollution too.\n",
            "There have been many agreements on ways to reduce the emission of greenhouse gases in the past.\n",
            "Now nations across the world have understood the urgency of the global warming issue, and are taking action to prevent further rising of global temperatures.\n",
            "We can drive less and use public transport instead.\n",
            "As individuals, we too have the responsibility in making our planet a safe abode for our future generations.\n"
          ]
        }
      ]
    }
  ]
}